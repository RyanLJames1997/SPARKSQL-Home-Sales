# SparkSQL-Home-Sales
Module 22 Challenge - UWA/edX Data Analytics Bootcamp

Github respository: [https://github.com/RyanLJames1997/SPARKSQL-Home-Sales](https://github.com/RyanLJames1997/SPARKSQL-Home-Sales)

## Introduction

### Goal
The overarching goal of this challenege is to use SparkSQL to determine key metrics within home sale data set.

### Repository Structure

The root directory contains
- The source code: `Final_Home_Sales.ipynb`.

### Dataset
Data for this dataset was generated by __edX Boot Camps LLC__.


## Approach
1. Read the CSV file to a Spark DataFrame
    - Set: `timestampFormat = "yyyy-MM-dd"`
    
2. Create a temporary view using: `createOrReplaceTempView()`

3. Write the SQL queries for the following questions:
    - What is the average price for a four bedroom house sold in each year roudned to two decimal places?
    - What is the average price of a home for each year the home was built that have 3 bedrooms and 3 bathrooms rounded to two decimal places?
    - What is the average price of a home for each year built that have 3 bedrooms, 3 bathrooms, with two floors, and are greater than or equal to 2,000 square feet rounded to two decimal places?

4. Determine the runtime for the following query:
    - What is the "view" rating for the average price of a home, rounded to two decimal places, where the homes are greater than or equal to $350,000?

5. Cache the temporary table and check if it cached correctly.

6. Run the same query as in Step 4, comparing the cached and uncached runtimes.
    - Runtimes:
        - Uncached = `1.22 seconds`
        - Cached = `0.59 seconds`
    - The runtime for the cached data is significantly quicker.
    - Please note: speed variation is dependent on the users machine. Furthermore, this was conducted through `Google Collab`.

7. Partition by the `date_built` field on the formatted parquet home sales data and create a temporary view.

8. Run the same query as in Step 4, with the partitioned parquet DataFrame, comparing the runtimes.
    - Runtime: `1.61 seconds`
    - The partitioned parquet data took almost 3 times longer than the cached version.
    - The `home_sales` dataset was partitioned by the `date_built` field and was queried by `view` instead.

9. Uncache the temporary table and check if it uncached correctly.
